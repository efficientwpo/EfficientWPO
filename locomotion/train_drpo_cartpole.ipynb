{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from a2c import A2CAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create Gym environment\n",
    "a2c_env = \"CartPole-v1\"\n",
    "env = gym.make(a2c_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if a2c_env == \"CartPole-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 1e-3\n",
    "\n",
    "agent = A2CAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 500\n",
    "max_steps = 500\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory.append((state, action, reward, next_state, done))\n",
    "        episode_reward += reward  \n",
    "        if done or step == max_steps:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n",
    "            break\n",
    "        state = next_state\n",
    "    agent.update(trajectory, 0)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "a2c_rewards = episode_rewards\n",
    "a2c_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/a2c/' + a2c_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((a2c_runtime, a2c_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRPO Agent (KL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 20.0\n",
      "Episode 1: 10.5\n",
      "Episode 2: 12.0\n",
      "Episode 3: 11.5\n",
      "Episode 4: 12.0\n",
      "Episode 5: 10.5\n",
      "Episode 6: 10.5\n",
      "Episode 7: 14.0\n",
      "Episode 8: 11.0\n",
      "Episode 9: 17.5\n",
      "Episode 10: 11.5\n",
      "Episode 11: 18.0\n",
      "Episode 12: 12.5\n",
      "Episode 13: 10.5\n",
      "Episode 14: 21.5\n",
      "Episode 15: 30.0\n",
      "Episode 16: 18.5\n",
      "Episode 17: 24.5\n",
      "Episode 18: 21.0\n",
      "Episode 19: 18.0\n",
      "Episode 20: 19.0\n",
      "Episode 21: 37.5\n",
      "Episode 22: 29.5\n",
      "Episode 23: 45.5\n",
      "Episode 24: 43.0\n",
      "Episode 25: 30.0\n",
      "Episode 26: 56.5\n",
      "Episode 27: 55.0\n",
      "Episode 28: 54.5\n",
      "Episode 29: 59.0\n",
      "Episode 30: 58.5\n",
      "Episode 31: 72.5\n",
      "Episode 32: 49.0\n",
      "Episode 33: 87.0\n",
      "Episode 34: 62.0\n",
      "Episode 35: 59.0\n",
      "Episode 36: 66.0\n",
      "Episode 37: 78.0\n",
      "Episode 38: 84.5\n",
      "Episode 39: 83.5\n",
      "Episode 40: 81.0\n",
      "Episode 41: 146.0\n",
      "Episode 42: 123.5\n",
      "Episode 43: 88.0\n",
      "Episode 44: 90.0\n",
      "Episode 45: 137.0\n",
      "Episode 46: 81.0\n",
      "Episode 47: 235.0\n",
      "Episode 48: 116.0\n",
      "Episode 49: 188.0\n",
      "Episode 50: 177.0\n",
      "Episode 51: 249.5\n",
      "Episode 52: 290.5\n",
      "Episode 53: 204.5\n",
      "Episode 54: 191.0\n",
      "Episode 55: 198.0\n",
      "Episode 56: 218.0\n",
      "Episode 57: 179.5\n",
      "Episode 58: 217.0\n",
      "Episode 59: 130.0\n",
      "Episode 60: 142.0\n",
      "Episode 61: 171.0\n",
      "Episode 62: 136.0\n",
      "Episode 63: 139.5\n",
      "Episode 64: 93.0\n",
      "Episode 65: 113.5\n",
      "Episode 66: 108.5\n",
      "Episode 67: 86.0\n",
      "Episode 68: 71.5\n",
      "Episode 69: 92.0\n",
      "Episode 70: 107.0\n",
      "Episode 71: 71.5\n",
      "Episode 72: 72.5\n",
      "Episode 73: 79.5\n",
      "Episode 74: 66.0\n",
      "Episode 75: 97.5\n",
      "Episode 76: 115.0\n",
      "Episode 77: 67.5\n",
      "Episode 78: 62.5\n",
      "Episode 79: 56.0\n",
      "Episode 80: 78.0\n",
      "Episode 81: 92.0\n",
      "Episode 82: 64.5\n",
      "Episode 83: 67.5\n",
      "Episode 84: 78.5\n",
      "Episode 85: 55.0\n",
      "Episode 86: 66.0\n",
      "Episode 87: 57.0\n",
      "Episode 88: 86.0\n",
      "Episode 89: 51.5\n",
      "Episode 90: 52.0\n",
      "Episode 91: 65.5\n",
      "Episode 92: 93.5\n",
      "Episode 93: 67.5\n",
      "Episode 94: 57.0\n",
      "Episode 95: 95.0\n",
      "Episode 96: 69.0\n",
      "Episode 97: 62.5\n",
      "Episode 98: 102.0\n",
      "Episode 99: 65.0\n",
      "Episode 100: 168.5\n",
      "Episode 101: 72.5\n",
      "Episode 102: 94.5\n",
      "Episode 103: 129.0\n",
      "Episode 104: 85.0\n",
      "Episode 105: 97.0\n",
      "Episode 106: 115.5\n",
      "Episode 107: 99.0\n",
      "Episode 108: 68.5\n",
      "Episode 109: 71.5\n",
      "Episode 110: 101.0\n",
      "Episode 111: 88.5\n",
      "Episode 112: 106.0\n",
      "Episode 113: 118.0\n",
      "Episode 114: 159.5\n",
      "Episode 115: 99.0\n",
      "Episode 116: 80.5\n",
      "Episode 117: 83.5\n",
      "Episode 118: 124.0\n",
      "Episode 119: 112.5\n",
      "Episode 120: 220.5\n",
      "Episode 121: 105.5\n",
      "Episode 122: 96.0\n",
      "Episode 123: 117.0\n",
      "Episode 124: 94.5\n",
      "Episode 125: 104.5\n",
      "Episode 126: 142.5\n",
      "Episode 127: 103.0\n",
      "Episode 128: 86.0\n",
      "Episode 129: 111.5\n",
      "Episode 130: 122.0\n",
      "Episode 131: 127.5\n",
      "Episode 132: 162.5\n",
      "Episode 133: 148.5\n",
      "Episode 134: 143.0\n",
      "Episode 135: 116.0\n",
      "Episode 136: 121.5\n",
      "Episode 137: 131.5\n",
      "Episode 138: 152.5\n",
      "Episode 139: 156.5\n",
      "Episode 140: 139.0\n",
      "Episode 141: 304.0\n",
      "Episode 142: 131.0\n",
      "Episode 143: 157.0\n",
      "Episode 144: 96.5\n",
      "Episode 145: 161.0\n",
      "Episode 146: 215.5\n",
      "Episode 147: 320.0\n",
      "Episode 148: 304.0\n",
      "Episode 149: 125.5\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from drpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create Gym environment\n",
    "kl_env = \"CartPole-v1\"\n",
    "env = gym.make(kl_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "\n",
    "# When the learning rate is large, policy neural network can overflow and lead to NaNs. \n",
    "# A possible fix is to reduce lr or increase beta to lower the learning rate.\n",
    "\n",
    "if kl_env == \"CartPole-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 1e-2\n",
    "    beta = 0.8\n",
    "\n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 150\n",
    "max_steps = 500\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    if episode == 0:\n",
    "        first_state = env.reset()\n",
    "    else:\n",
    "        first_state = state\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps-1:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "        \n",
    "    avg_episode_reward = episode_reward/env.action_space.n        \n",
    "    # add randomness for better exploration\n",
    "    if (avg_episode_reward <= 300) and (episode % 10 == 0):\n",
    "        state_adv[0] += (np.random.random()-0.5)*0.5\n",
    "        state_adv[1] += (np.random.random()-0.5)*0.5\n",
    "    \n",
    "    state_adv[0] += 0.5\n",
    "    \n",
    "    # restart the agent if stuck\n",
    "    if (episode >= 5) and (avg_episode_reward <= 15):\n",
    "        agent = DRTRPOAgent(env, gamma, lr)   \n",
    "    \n",
    "    policy_loss = agent.compute_policy_loss_kl(first_state, state_adv, beta)\n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_kl_rewards = episode_rewards\n",
    "dr_trpo_kl_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_kl/' + kl_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_kl_runtime, dr_trpo_kl_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRPO Agent (Wasserstein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 20.0\n",
      "Episode 1: 15.0\n",
      "Episode 2: 13.5\n",
      "Episode 3: 12.5\n",
      "Episode 4: 11.0\n",
      "Episode 5: 11.0\n",
      "Episode 6: 15.5\n",
      "Episode 7: 20.0\n",
      "Episode 8: 20.5\n",
      "Episode 9: 20.5\n",
      "Episode 10: 40.0\n",
      "Episode 11: 50.5\n",
      "Episode 12: 86.5\n",
      "Episode 13: 75.0\n",
      "Episode 14: 62.5\n",
      "Episode 15: 56.5\n",
      "Episode 16: 46.5\n",
      "Episode 17: 75.0\n",
      "Episode 18: 46.0\n",
      "Episode 19: 52.5\n",
      "Episode 20: 50.0\n",
      "Episode 21: 35.0\n",
      "Episode 22: 26.0\n",
      "Episode 23: 27.0\n",
      "Episode 24: 17.5\n",
      "Episode 25: 16.5\n",
      "Episode 26: 14.5\n",
      "Episode 27: 20.5\n",
      "Episode 28: 18.0\n",
      "Episode 29: 18.0\n",
      "Episode 30: 19.0\n",
      "Episode 31: 24.0\n",
      "Episode 32: 19.0\n",
      "Episode 33: 27.0\n",
      "Episode 34: 32.0\n",
      "Episode 35: 59.5\n",
      "Episode 36: 71.5\n",
      "Episode 37: 63.0\n",
      "Episode 38: 105.0\n",
      "Episode 39: 41.0\n",
      "Episode 40: 33.5\n",
      "Episode 41: 30.0\n",
      "Episode 42: 26.5\n",
      "Episode 43: 26.5\n",
      "Episode 44: 19.0\n",
      "Episode 45: 28.0\n",
      "Episode 46: 27.0\n",
      "Episode 47: 31.5\n",
      "Episode 48: 38.0\n",
      "Episode 49: 44.0\n",
      "Episode 50: 47.0\n",
      "Episode 51: 83.5\n",
      "Episode 52: 142.5\n",
      "Episode 53: 95.0\n",
      "Episode 54: 76.5\n",
      "Episode 55: 93.5\n",
      "Episode 56: 53.0\n",
      "Episode 57: 91.5\n",
      "Episode 58: 74.5\n",
      "Episode 59: 57.5\n",
      "Episode 60: 49.5\n",
      "Episode 61: 50.5\n",
      "Episode 62: 44.0\n",
      "Episode 63: 56.0\n",
      "Episode 64: 46.0\n",
      "Episode 65: 41.5\n",
      "Episode 66: 49.0\n",
      "Episode 67: 45.0\n",
      "Episode 68: 45.0\n",
      "Episode 69: 48.5\n",
      "Episode 70: 40.5\n",
      "Episode 71: 48.5\n",
      "Episode 72: 43.0\n",
      "Episode 73: 68.0\n",
      "Episode 74: 75.0\n",
      "Episode 75: 99.5\n",
      "Episode 76: 121.5\n",
      "Episode 77: 149.0\n",
      "Episode 78: 135.5\n",
      "Episode 79: 164.5\n",
      "Episode 80: 187.5\n",
      "Episode 81: 246.0\n",
      "Episode 82: 179.5\n",
      "Episode 83: 299.5\n",
      "Episode 84: 273.0\n",
      "Episode 85: 245.5\n",
      "Episode 86: 195.0\n",
      "Episode 87: 239.0\n",
      "Episode 88: 183.5\n",
      "Episode 89: 221.0\n",
      "Episode 90: 250.5\n",
      "Episode 91: 183.5\n",
      "Episode 92: 198.0\n",
      "Episode 93: 194.5\n",
      "Episode 94: 187.5\n",
      "Episode 95: 200.5\n",
      "Episode 96: 194.0\n",
      "Episode 97: 183.0\n",
      "Episode 98: 171.0\n",
      "Episode 99: 182.0\n",
      "Episode 100: 176.0\n",
      "Episode 101: 155.0\n",
      "Episode 102: 177.5\n",
      "Episode 103: 191.5\n",
      "Episode 104: 183.0\n",
      "Episode 105: 179.0\n",
      "Episode 106: 170.0\n",
      "Episode 107: 180.0\n",
      "Episode 108: 176.0\n",
      "Episode 109: 183.0\n",
      "Episode 110: 179.5\n",
      "Episode 111: 180.5\n",
      "Episode 112: 176.5\n",
      "Episode 113: 164.0\n",
      "Episode 114: 169.0\n",
      "Episode 115: 175.0\n",
      "Episode 116: 184.5\n",
      "Episode 117: 188.5\n",
      "Episode 118: 166.0\n",
      "Episode 119: 160.5\n",
      "Episode 120: 172.5\n",
      "Episode 121: 171.5\n",
      "Episode 122: 190.5\n",
      "Episode 123: 164.5\n",
      "Episode 124: 193.0\n",
      "Episode 125: 180.0\n",
      "Episode 126: 173.0\n",
      "Episode 127: 154.5\n",
      "Episode 128: 162.0\n",
      "Episode 129: 187.5\n",
      "Episode 130: 171.5\n",
      "Episode 131: 178.0\n",
      "Episode 132: 180.0\n",
      "Episode 133: 191.5\n",
      "Episode 134: 157.5\n",
      "Episode 135: 154.5\n",
      "Episode 136: 185.0\n",
      "Episode 137: 183.0\n",
      "Episode 138: 186.5\n",
      "Episode 139: 150.5\n",
      "Episode 140: 158.0\n",
      "Episode 141: 175.0\n",
      "Episode 142: 182.5\n",
      "Episode 143: 184.5\n",
      "Episode 144: 188.5\n",
      "Episode 145: 165.5\n",
      "Episode 146: 170.0\n",
      "Episode 147: 168.0\n",
      "Episode 148: 181.0\n",
      "Episode 149: 182.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from drpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "wass_env = \"CartPole-v1\"\n",
    "# Create Gym environment\n",
    "env = gym.make(wass_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if wass_env == \"CartPole-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 1e-2\n",
    "    \n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 150\n",
    "max_steps = 500\n",
    "total_adv_diff = 0\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    if episode == 0:\n",
    "        first_state = env.reset()\n",
    "    else:\n",
    "        first_state = state\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps-1:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "    \n",
    "    total_adv_diff += abs(state_adv[1] - state_adv[0])\n",
    "    # larger beta, better stability; smaller beta, better exploration\n",
    "    beta = total_adv_diff/episode \n",
    "    beta += np.random.random()*0.3-0.1\n",
    "    \n",
    "    avg_episode_reward = episode_reward/env.action_space.n\n",
    "    # add randomness for better exploration\n",
    "    if (episode % 10 == 0) and (avg_episode_reward <= 350): \n",
    "        state_adv[0] += (np.random.random()-0.5)*0.5\n",
    "        state_adv[1] += (np.random.random()-0.5)*0.5\n",
    "        \n",
    "    state_adv[0] += 0.5\n",
    "        \n",
    "    # restart the agent if stuck\n",
    "    if (episode >= 5) and (avg_episode_reward <= 15):\n",
    "        agent = DRTRPOAgent(env, gamma, lr)\n",
    "    \n",
    "    policy_loss = agent.compute_policy_loss_wass(first_state, state_adv, beta)\n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_wass_rewards = episode_rewards\n",
    "dr_trpo_wass_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_wass/' + wass_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_wass_runtime, dr_trpo_wass_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRPO Agent (Sinkhorn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from drpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sink_env = \"CartPole-v1\"\n",
    "# Create Gym environment\n",
    "env = gym.make(sink_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if wass_env == \"CartPole-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 1e-2\n",
    "    \n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 200\n",
    "max_steps = 500\n",
    "total_adv_diff = 0\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    if episode == 0:\n",
    "        first_state = env.reset()\n",
    "    else:\n",
    "        first_state = state\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps-1:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "\n",
    "    total_adv_diff += abs(state_adv[1] - state_adv[0])\n",
    "    # larger beta, better stability; smaller beta, better exploration\n",
    "    beta = total_adv_diff/episode \n",
    "    beta += np.random.random()*0.3-0.1\n",
    "    \n",
    "    avg_episode_reward = episode_reward/env.action_space.n\n",
    "    # add randomness for better exploration\n",
    "    if (episode % 10 == 0) and (avg_episode_reward <= 350): \n",
    "        state_adv[0] += (np.random.random()-0.5)*0.5\n",
    "        state_adv[1] += (np.random.random()-0.5)*0.5\n",
    "        \n",
    "    # restart the agent if stuck\n",
    "    if (episode >= 5) and (avg_episode_reward <= 10):\n",
    "        agent = DRTRPOAgent(env, gamma, lr)\n",
    "    \n",
    "    beta = 50\n",
    "    policy_loss = agent.compute_policy_loss_sinkhorn(first_state, state_adv, beta)\n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_sink_rewards = episode_rewards\n",
    "dr_trpo_sink_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_sink/' + sink_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_sink_runtime, dr_trpo_sink_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
