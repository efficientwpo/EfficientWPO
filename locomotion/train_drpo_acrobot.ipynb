{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from a2c import A2CAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create Gym environment\n",
    "a2c_env = \"Acrobot-v1\"\n",
    "env = gym.make(a2c_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if a2c_env == \"Acrobot-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 7e-4\n",
    "\n",
    "agent = A2CAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 300\n",
    "max_steps = 500\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory.append((state, action, reward, next_state, done))\n",
    "        episode_reward += reward  \n",
    "        if done or step == max_steps:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n",
    "            break\n",
    "        state = next_state\n",
    "    agent.update(trajectory, 0)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "a2c_rewards = episode_rewards\n",
    "a2c_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/a2c/' + a2c_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((a2c_runtime, a2c_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRPO Agent (KL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: -500.0\n",
      "Episode 1: -500.0\n",
      "Episode 2: -500.0\n",
      "Episode 3: -500.0\n",
      "Episode 4: -500.0\n",
      "Episode 5: -500.0\n",
      "Episode 6: -500.0\n",
      "Episode 7: -500.0\n",
      "Episode 8: -500.0\n",
      "Episode 9: -500.0\n",
      "Episode 10: -500.0\n",
      "Episode 11: -500.0\n",
      "Episode 12: -500.0\n",
      "Episode 13: -500.0\n",
      "Episode 14: -500.0\n",
      "Episode 15: -500.0\n",
      "Episode 16: -500.0\n",
      "Episode 17: -500.0\n",
      "Episode 18: -500.0\n",
      "Episode 19: -500.0\n",
      "Episode 20: -462.6666666666667\n",
      "Episode 21: -500.0\n",
      "Episode 22: -500.0\n",
      "Episode 23: -500.0\n",
      "Episode 24: -500.0\n",
      "Episode 25: -500.0\n",
      "Episode 26: -500.0\n",
      "Episode 27: -410.3333333333333\n",
      "Episode 28: -500.0\n",
      "Episode 29: -500.0\n",
      "Episode 30: -500.0\n",
      "Episode 31: -500.0\n",
      "Episode 32: -202.33333333333334\n",
      "Episode 33: -140.0\n",
      "Episode 34: -150.0\n",
      "Episode 35: -185.66666666666666\n",
      "Episode 36: -170.0\n",
      "Episode 37: -159.66666666666666\n",
      "Episode 38: -217.66666666666666\n",
      "Episode 39: -200.33333333333334\n",
      "Episode 40: -188.0\n",
      "Episode 41: -216.66666666666666\n",
      "Episode 42: -176.33333333333334\n",
      "Episode 43: -185.33333333333334\n",
      "Episode 44: -198.66666666666666\n",
      "Episode 45: -230.33333333333334\n",
      "Episode 46: -296.6666666666667\n",
      "Episode 47: -245.0\n",
      "Episode 48: -249.0\n",
      "Episode 49: -199.0\n",
      "Episode 50: -242.0\n",
      "Episode 51: -208.33333333333334\n",
      "Episode 52: -231.33333333333334\n",
      "Episode 53: -219.33333333333334\n",
      "Episode 54: -228.66666666666666\n",
      "Episode 55: -219.0\n",
      "Episode 56: -209.0\n",
      "Episode 57: -326.6666666666667\n",
      "Episode 58: -182.66666666666666\n",
      "Episode 59: -229.0\n",
      "Episode 60: -249.33333333333334\n",
      "Episode 61: -224.66666666666666\n",
      "Episode 62: -232.0\n",
      "Episode 63: -251.33333333333334\n",
      "Episode 64: -241.33333333333334\n",
      "Episode 65: -250.33333333333334\n",
      "Episode 66: -267.6666666666667\n",
      "Episode 67: -246.0\n",
      "Episode 68: -255.0\n",
      "Episode 69: -284.6666666666667\n",
      "Episode 70: -283.0\n",
      "Episode 71: -279.0\n",
      "Episode 72: -264.3333333333333\n",
      "Episode 73: -298.0\n",
      "Episode 74: -212.0\n",
      "Episode 75: -248.33333333333334\n",
      "Episode 76: -225.0\n",
      "Episode 77: -236.66666666666666\n",
      "Episode 78: -269.0\n",
      "Episode 79: -254.66666666666666\n",
      "Episode 80: -306.0\n",
      "Episode 81: -205.0\n",
      "Episode 82: -308.3333333333333\n",
      "Episode 83: -216.0\n",
      "Episode 84: -222.33333333333334\n",
      "Episode 85: -256.0\n",
      "Episode 86: -231.33333333333334\n",
      "Episode 87: -242.66666666666666\n",
      "Episode 88: -235.33333333333334\n",
      "Episode 89: -251.33333333333334\n",
      "Episode 90: -226.0\n",
      "Episode 91: -238.66666666666666\n",
      "Episode 92: -270.3333333333333\n",
      "Episode 93: -218.66666666666666\n",
      "Episode 94: -221.0\n",
      "Episode 95: -261.6666666666667\n",
      "Episode 96: -215.0\n",
      "Episode 97: -227.0\n",
      "Episode 98: -241.0\n",
      "Episode 99: -214.33333333333334\n",
      "Episode 100: -203.0\n",
      "Episode 101: -230.33333333333334\n",
      "Episode 102: -239.66666666666666\n",
      "Episode 103: -316.6666666666667\n",
      "Episode 104: -184.33333333333334\n",
      "Episode 105: -211.66666666666666\n",
      "Episode 106: -190.0\n",
      "Episode 107: -268.3333333333333\n",
      "Episode 108: -214.66666666666666\n",
      "Episode 109: -206.0\n",
      "Episode 110: -215.33333333333334\n",
      "Episode 111: -228.33333333333334\n",
      "Episode 112: -220.66666666666666\n",
      "Episode 113: -270.0\n",
      "Episode 114: -193.0\n",
      "Episode 115: -330.3333333333333\n",
      "Episode 116: -233.33333333333334\n",
      "Episode 117: -230.0\n",
      "Episode 118: -186.0\n",
      "Episode 119: -239.33333333333334\n",
      "Episode 120: -224.33333333333334\n",
      "Episode 121: -217.66666666666666\n",
      "Episode 122: -261.3333333333333\n",
      "Episode 123: -190.0\n",
      "Episode 124: -198.0\n",
      "Episode 125: -255.66666666666666\n",
      "Episode 126: -236.0\n",
      "Episode 127: -253.33333333333334\n",
      "Episode 128: -198.0\n",
      "Episode 129: -176.33333333333334\n",
      "Episode 130: -319.3333333333333\n",
      "Episode 131: -211.66666666666666\n",
      "Episode 132: -254.0\n",
      "Episode 133: -264.3333333333333\n",
      "Episode 134: -242.66666666666666\n",
      "Episode 135: -243.66666666666666\n",
      "Episode 136: -272.6666666666667\n",
      "Episode 137: -241.66666666666666\n",
      "Episode 138: -232.33333333333334\n",
      "Episode 139: -194.0\n",
      "Episode 140: -244.33333333333334\n",
      "Episode 141: -212.66666666666666\n",
      "Episode 142: -329.0\n",
      "Episode 143: -240.33333333333334\n",
      "Episode 144: -228.66666666666666\n",
      "Episode 145: -296.0\n",
      "Episode 146: -261.3333333333333\n",
      "Episode 147: -214.66666666666666\n",
      "Episode 148: -273.3333333333333\n",
      "Episode 149: -246.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from drpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create Gym environment\n",
    "kl_env = \"Acrobot-v1\"\n",
    "env = gym.make(kl_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "\n",
    "# When the learning rate is large, policy neural network can overflow and lead to NaNs. \n",
    "# A possible fix is to reduce lr or increase beta to lower the learning rate.\n",
    "\n",
    "if kl_env == \"Acrobot-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 1e-2\n",
    "    beta = 0.8\n",
    "\n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 150\n",
    "max_steps = 500\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    if episode == 0:\n",
    "        first_state = env.reset()\n",
    "    else:\n",
    "        first_state = state\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "        \n",
    "    avg_episode_reward = episode_reward/env.action_space.n\n",
    "    # add randomness for better exploration\n",
    "    beta += np.random.random()*0.1\n",
    "    if(state_adv[0] == state_adv[1]) and (state_adv[1] == state_adv[2]) and avg_episode_reward  <= -490:\n",
    "        state_adv[0] += (np.random.random()-0.5)*2\n",
    "        state_adv[1] += (np.random.random()-0.5)*2\n",
    "        state_adv[2] += (np.random.random()-0.5)*2\n",
    "        \n",
    "    state_adv[1] += 5 \n",
    "    \n",
    "    # restart the agent if stuck\n",
    "    if (episode >= 5) and (avg_episode_reward <= -490):\n",
    "        agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "    policy_loss = agent.compute_policy_loss_kl(first_state, state_adv, beta)\n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_kl_rewards = episode_rewards\n",
    "dr_trpo_kl_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_kl/' + kl_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_kl_runtime, dr_trpo_kl_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRPO Agent (Wasserstein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: -500.0\n",
      "Episode 1: -500.0\n",
      "Episode 2: -496.0\n",
      "Episode 3: -439.6666666666667\n",
      "Episode 4: -476.3333333333333\n",
      "Episode 5: -407.0\n",
      "Episode 6: -500.0\n",
      "Episode 7: -500.0\n",
      "Episode 8: -500.0\n",
      "Episode 9: -500.0\n",
      "Episode 10: -500.0\n",
      "Episode 11: -193.66666666666666\n",
      "Episode 12: -174.33333333333334\n",
      "Episode 13: -152.66666666666666\n",
      "Episode 14: -159.33333333333334\n",
      "Episode 15: -120.33333333333333\n",
      "Episode 16: -147.33333333333334\n",
      "Episode 17: -131.33333333333334\n",
      "Episode 18: -152.66666666666666\n",
      "Episode 19: -130.0\n",
      "Episode 20: -134.33333333333334\n",
      "Episode 21: -145.33333333333334\n",
      "Episode 22: -136.66666666666666\n",
      "Episode 23: -147.33333333333334\n",
      "Episode 24: -131.33333333333334\n",
      "Episode 25: -137.33333333333334\n",
      "Episode 26: -126.33333333333333\n",
      "Episode 27: -141.66666666666666\n",
      "Episode 28: -135.33333333333334\n",
      "Episode 29: -126.33333333333333\n",
      "Episode 30: -132.33333333333334\n",
      "Episode 31: -161.33333333333334\n",
      "Episode 32: -156.66666666666666\n",
      "Episode 33: -138.66666666666666\n",
      "Episode 34: -136.0\n",
      "Episode 35: -120.66666666666667\n",
      "Episode 36: -132.0\n",
      "Episode 37: -134.66666666666666\n",
      "Episode 38: -148.66666666666666\n",
      "Episode 39: -138.66666666666666\n",
      "Episode 40: -127.33333333333333\n",
      "Episode 41: -160.66666666666666\n",
      "Episode 42: -142.33333333333334\n",
      "Episode 43: -119.66666666666667\n",
      "Episode 44: -129.33333333333334\n",
      "Episode 45: -116.66666666666667\n",
      "Episode 46: -161.33333333333334\n",
      "Episode 47: -134.33333333333334\n",
      "Episode 48: -135.66666666666666\n",
      "Episode 49: -171.66666666666666\n",
      "Episode 50: -148.33333333333334\n",
      "Episode 51: -133.0\n",
      "Episode 52: -116.0\n",
      "Episode 53: -125.0\n",
      "Episode 54: -142.0\n",
      "Episode 55: -152.33333333333334\n",
      "Episode 56: -128.33333333333334\n",
      "Episode 57: -120.66666666666667\n",
      "Episode 58: -119.0\n",
      "Episode 59: -143.66666666666666\n",
      "Episode 60: -169.0\n",
      "Episode 61: -140.33333333333334\n",
      "Episode 62: -121.66666666666667\n",
      "Episode 63: -145.33333333333334\n",
      "Episode 64: -132.0\n",
      "Episode 65: -123.0\n",
      "Episode 66: -116.0\n",
      "Episode 67: -137.0\n",
      "Episode 68: -138.66666666666666\n",
      "Episode 69: -140.33333333333334\n",
      "Episode 70: -140.66666666666666\n",
      "Episode 71: -135.0\n",
      "Episode 72: -122.0\n",
      "Episode 73: -188.33333333333334\n",
      "Episode 74: -120.0\n",
      "Episode 75: -146.66666666666666\n",
      "Episode 76: -124.66666666666667\n",
      "Episode 77: -154.0\n",
      "Episode 78: -142.33333333333334\n",
      "Episode 79: -147.0\n",
      "Episode 80: -152.33333333333334\n",
      "Episode 81: -179.66666666666666\n",
      "Episode 82: -144.0\n",
      "Episode 83: -123.66666666666667\n",
      "Episode 84: -126.66666666666667\n",
      "Episode 85: -138.33333333333334\n",
      "Episode 86: -127.0\n",
      "Episode 87: -127.33333333333333\n",
      "Episode 88: -149.66666666666666\n",
      "Episode 89: -125.0\n",
      "Episode 90: -137.33333333333334\n",
      "Episode 91: -135.0\n",
      "Episode 92: -114.33333333333333\n",
      "Episode 93: -129.66666666666666\n",
      "Episode 94: -131.0\n",
      "Episode 95: -127.66666666666667\n",
      "Episode 96: -162.33333333333334\n",
      "Episode 97: -131.33333333333334\n",
      "Episode 98: -157.66666666666666\n",
      "Episode 99: -151.66666666666666\n",
      "Episode 100: -136.66666666666666\n",
      "Episode 101: -176.0\n",
      "Episode 102: -146.66666666666666\n",
      "Episode 103: -143.33333333333334\n",
      "Episode 104: -125.0\n",
      "Episode 105: -132.33333333333334\n",
      "Episode 106: -130.33333333333334\n",
      "Episode 107: -130.0\n",
      "Episode 108: -144.0\n",
      "Episode 109: -133.66666666666666\n",
      "Episode 110: -130.0\n",
      "Episode 111: -131.33333333333334\n",
      "Episode 112: -167.33333333333334\n",
      "Episode 113: -183.33333333333334\n",
      "Episode 114: -157.33333333333334\n",
      "Episode 115: -124.33333333333333\n",
      "Episode 116: -137.66666666666666\n",
      "Episode 117: -151.66666666666666\n",
      "Episode 118: -122.0\n",
      "Episode 119: -122.0\n",
      "Episode 120: -147.66666666666666\n",
      "Episode 121: -136.33333333333334\n",
      "Episode 122: -148.33333333333334\n",
      "Episode 123: -138.33333333333334\n",
      "Episode 124: -122.33333333333333\n",
      "Episode 125: -121.33333333333333\n",
      "Episode 126: -116.33333333333333\n",
      "Episode 127: -148.33333333333334\n",
      "Episode 128: -122.66666666666667\n",
      "Episode 129: -118.0\n",
      "Episode 130: -131.66666666666666\n",
      "Episode 131: -125.33333333333333\n",
      "Episode 132: -104.33333333333333\n",
      "Episode 133: -135.33333333333334\n",
      "Episode 134: -123.33333333333333\n",
      "Episode 135: -123.0\n",
      "Episode 136: -133.0\n",
      "Episode 137: -126.0\n",
      "Episode 138: -155.66666666666666\n",
      "Episode 139: -120.33333333333333\n",
      "Episode 140: -156.66666666666666\n",
      "Episode 141: -127.33333333333333\n",
      "Episode 142: -119.33333333333333\n",
      "Episode 143: -183.33333333333334\n",
      "Episode 144: -148.33333333333334\n",
      "Episode 145: -158.33333333333334\n",
      "Episode 146: -139.33333333333334\n",
      "Episode 147: -167.66666666666666\n",
      "Episode 148: -234.66666666666666\n",
      "Episode 149: -319.6666666666667\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from drpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "wass_env = \"Acrobot-v1\"\n",
    "# Create Gym environment\n",
    "env = gym.make(wass_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if wass_env == \"Acrobot-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 1e-2\n",
    "    \n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 150\n",
    "max_steps = 500\n",
    "total_adv_diff = 0\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    if episode == 0:\n",
    "        first_state = env.reset()\n",
    "    else:\n",
    "        first_state = state\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "    \n",
    "    avg_episode_reward = episode_reward/env.action_space.n\n",
    "    # add randomness for better exploration \n",
    "    if(state_adv[0] == state_adv[1]) and (state_adv[1] == state_adv[2]) and avg_episode_reward < -490:\n",
    "        state_adv[0] += (np.random.random()-0.5)*2\n",
    "        state_adv[1] += (np.random.random()-0.5)*2\n",
    "        state_adv[2] += (np.random.random()-0.5)*2\n",
    "\n",
    "    state_adv[1] += 5\n",
    "    \n",
    "    # restart the agent if stuck\n",
    "    if (episode >= 5) and (avg_episode_reward <= -490):\n",
    "        agent = DRTRPOAgent(env, gamma, lr)\n",
    "        \n",
    "    total_adv_diff += max(abs(state_adv[1] - state_adv[0]), abs(state_adv[2] - state_adv[0]), abs(state_adv[2] - state_adv[1]))\n",
    "    beta = total_adv_diff/episode\n",
    "    policy_loss = agent.compute_policy_loss_wass(first_state, state_adv, beta)\n",
    "\n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_wass_rewards = episode_rewards\n",
    "dr_trpo_wass_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_wass/' + wass_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_wass_runtime, dr_trpo_wass_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRPO Agent (Sinkhorn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from drpo import DRTRPOAgent \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sink_env = \"Acrobot-v1\"\n",
    "# Create Gym environment\n",
    "env = gym.make(sink_env)\n",
    "\n",
    "# Check agent class for initialization parameters and initialize agent\n",
    "if sink_env == \"Acrobot-v1\":\n",
    "    gamma = 0.95\n",
    "    lr = 1e-2\n",
    "    \n",
    "agent = DRTRPOAgent(env, gamma, lr)\n",
    "\n",
    "# Define training parameters\n",
    "max_episodes = 150\n",
    "max_steps = 500\n",
    "total_adv_diff = 0\n",
    "\n",
    "episode_rewards = []\n",
    "run_time = []\n",
    "start_time = time.time()\n",
    "for episode in range(max_episodes):\n",
    "    if episode == 0:\n",
    "        first_state = env.reset()\n",
    "    else:\n",
    "        first_state = state\n",
    "    state_adv = []\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # loop through the first action\n",
    "    for i in range(env.action_space.n):\n",
    "        env.reset()raakute\n",
    "        state = first_state\n",
    "        action = i\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if step != 0:\n",
    "                action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            episode_reward += reward  \n",
    "            if done or step == max_steps:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        adv, value_loss = agent.compute_adv_mc(trajectory)\n",
    "        state_adv.append(adv[0])\n",
    "        total_value_loss += value_loss\n",
    "    \n",
    "    avg_episode_reward = episode_reward/env.action_space.n\n",
    "    # add randomness for better exploration \n",
    "    if(state_adv[0] == state_adv[1]) and (state_adv[1] == state_adv[2]) and avg_episode_reward < -490:\n",
    "        state_adv[0] += (np.random.random()-0.5)*2\n",
    "        state_adv[1] += (np.random.random()-0.5)*2\n",
    "        state_adv[2] += (np.random.random()-0.5)*2\n",
    "        \n",
    "    state_adv[1] += 5\n",
    "\n",
    "    # restart the agent if stuck\n",
    "    if (episode >= 5) and (avg_episode_reward <= -490):\n",
    "        agent = DRTRPOAgent(env, gamma, lr)\n",
    "        \n",
    "    total_adv_diff += max(abs(state_adv[1] - state_adv[0]), abs(state_adv[2] - state_adv[0]), abs(state_adv[2] - state_adv[1]))\n",
    "    beta = 40\n",
    "    policy_loss = agent.compute_policy_loss_sinkhorn(first_state, state_adv, beta)\n",
    "\n",
    "        \n",
    "    agent.update(value_loss, policy_loss)\n",
    "    elapse = time.time() - start_time\n",
    "    run_time.append(elapse)\n",
    "    \n",
    "    episode_rewards.append(avg_episode_reward)\n",
    "    print(\"Episode \" + str(episode) + \": \" + str(avg_episode_reward))\n",
    "\n",
    "dr_trpo_sink_rewards = episode_rewards\n",
    "dr_trpo_sink_runtime = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './log_files/dr_trpo_sink/' + sink_env + '-' + str(time.time()) + '.csv' \n",
    "out = np.column_stack((dr_trpo_sink_runtime, dr_trpo_sink_rewards))\n",
    "with open(name, 'ab') as f:\n",
    "    np.savetxt(f, out, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
